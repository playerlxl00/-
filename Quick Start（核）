# =============================
# 常见核函数实现
# =============================


def linear_kernel(x, z):
    return np.dot(x, z)    # 普通线性SVM，不做升维


def polynomial_kernel(x, z, degree=3, coef0=1.0):
    return (np.dot(x, z) + coef0) ** degree    # 通过多项式映射，将输入数据隐式地映射到高维空间


def rbf_kernel(x, z, gamma=0.5):
    diff = x - z
    return np.exp(-gamma * np.dot(diff, diff))    # 将数据映射到无限维空间，能处理复杂非线性问题。gamma 控制相似度的衰减速度


# =============================
# 核 SVM（Dual, SMO-like）
# =============================

class KernelSVM:
    """
    基于对偶问题的核 SVM（简化 SMO）
    仅用于教学与理解核技巧
    """
    def __init__(self, kernel=rbf_kernel, C=1.0, epochs=100):
        self.kernel = kernel
        self.C = C
        self.epochs = epochs
        self.alpha = None
        self.b = 0.0
        self.X = None
        self.y = None

    def fit(self, X, y):
        n_samples = X.shape[0]
        self.X = X
        self.y = y
        self.alpha = np.zeros(n_samples)

        # 预计算核矩阵
        K = np.zeros((n_samples, n_samples))
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = self.kernel(X[i], X[j])

        for _ in range(self.epochs):
            for i in range(n_samples):
                # 计算预测误差
                decision = np.sum(self.alpha * y * K[:, i]) + self.b
                error = decision - y[i]

                # KKT 条件违反时更新
                if (y[i] * error < -1e-3 and self.alpha[i] < self.C) or \
                   (y[i] * error > 1e-3 and self.alpha[i] > 0):
                    j = np.random.randint(0, n_samples)
                    if j == i:
                        continue

                    decision_j = np.sum(self.alpha * y * K[:, j]) + self.b
                    error_j = decision_j - y[j]

                    alpha_i_old = self.alpha[i]
                    alpha_j_old = self.alpha[j]

                    # 计算上下界
                    if y[i] != y[j]:
                        L = max(0, self.alpha[j] - self.alpha[i])
                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])
                    else:
                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)
                        H = min(self.C, self.alpha[i] + self.alpha[j])

                    if L == H:
                        continue

                    eta = 2 * K[i, j] - K[i, i] - K[j, j]
                    if eta >= 0:
                        continue

                    self.alpha[j] -= y[j] * (error - error_j) / eta
                    self.alpha[j] = np.clip(self.alpha[j], L, H)

                    self.alpha[i] += y[i] * y[j] * (alpha_j_old - self.alpha[j])

                    # 更新偏置 b
                    b1 = self.b - error \
                         - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] \
                         - y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]

                    b2 = self.b - error_j \
                         - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] \
                         - y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]

                    if 0 < self.alpha[i] < self.C:
                        self.b = b1
                    elif 0 < self.alpha[j] < self.C:
                        self.b = b2
                    else:
                        self.b = 0.5 * (b1 + b2)

    def predict(self, X):
        y_pred = []
        for x in X:
            s = 0
            for alpha_i, y_i, x_i in zip(self.alpha, self.y, self.X):
                s += alpha_i * y_i * self.kernel(x_i, x)
            y_pred.append(s + self.b)
        return np.sign(y_pred)
