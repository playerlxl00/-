import numpy as np
import matplotlib.pyplot as plt

# =============================
# 常见核函数实现
# =============================


def linear_kernel(x, z):
    return np.dot(x, z)    # 普通线性SVM，不做升维


def polynomial_kernel(x, z, degree=3, coef0=1.0):
    return (np.dot(x, z) + coef0) ** degree    # 通过多项式映射，将输入数据隐式地映射到高维空间


def rbf_kernel(x, z, gamma=0.5):
    diff = x - z
    return np.exp(-gamma * np.dot(diff, diff))    # 将数据映射到无限维空间，能处理复杂非线性问题。gamma 控制相似度的衰减速度


# =============================
# 核 SVM（Dual, SMO-like）
# =============================

class KernelSVM:
    """
    基于对偶问题的核 SVM（简化 SMO）　　　　# SMO：每次选择两个 α 一起更新
    """
    def __init__(self, kernel=rbf_kernel, C=1.0, epochs=100):
        self.kernel = kernel
        self.C = C
        self.epochs = epochs
        self.alpha = None
        self.b = 0.0
        self.X = None
        self.y = None

    def fit(self, X, y):
        n_samples = X.shape[0]
        self.X = X
        self.y = y
        self.alpha = np.zeros(n_samples)

        # 预计算核矩阵
        K = np.zeros((n_samples, n_samples))
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = self.kernel(X[i], X[j])

        for _ in range(self.epochs):
            for i in range(n_samples):
                # 计算预测误差
                decision = np.sum(self.alpha * y * K[:, i]) + self.b
                error = decision - y[i]

                # KKT 条件违反时更新
                if (y[i] * error < -1e-3 and self.alpha[i] < self.C) or \
                   (y[i] * error > 1e-3 and self.alpha[i] > 0):
                    j = np.random.randint(0, n_samples)
                    if j == i:
                        continue

                    decision_j = np.sum(self.alpha * y * K[:, j]) + self.b
                    error_j = decision_j - y[j]

                    alpha_i_old = self.alpha[i]
                    alpha_j_old = self.alpha[j]

                    # 计算上下界：保证更新后的 α_i 和 α_j 仍满足约束条件：0≤alphai, alphaj≤C，∑alphaiyi=0，因为假如alphaj太大，alphai可能越界，所以需要限制保证约束始终满足
                    if y[i] != y[j]:
                        L = max(0, self.alpha[j] - self.alpha[i])
                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])
                    else:
                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)
                        H = min(self.C, self.alpha[i] + self.alpha[j])

                    if L == H:
                        continue

                    # 直接对对偶函数关于alphaj求两次导得到eta，即曲率
                    eta = 2 * K[i, j] - K[i, i] - K[j, j]
                    if eta >= 0:
                        continue

                    self.alpha[j] -= y[j] * (error - error_j) / eta
                    self.alpha[j] = np.clip(self.alpha[j], L, H)

                    self.alpha[i] += y[i] * y[j] * (alpha_j_old - self.alpha[j])

                    # 更新偏置 b
                    b1 = self.b - error \
                         - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] \
                         - y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]

                    b2 = self.b - error_j \
                         - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] \
                         - y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]

                    if 0 < self.alpha[i] < self.C:
                        self.b = b1
                    elif 0 < self.alpha[j] < self.C:
                        self.b = b2
                    else:
                        self.b = 0.5 * (b1 + b2)

    def predict(self, X):
        y_pred = []
        for x in X:
            s = 0
            for alpha_i, y_i, x_i in zip(self.alpha, self.y, self.X):
                s += alpha_i * y_i * self.kernel(x_i, x)
            y_pred.append(s + self.b)
        return np.sign(y_pred)

# -----------------------------
# 1. 示例数据
# -----------------------------
# 简单非线性数据：两个圆环
np.random.seed(0)
n_samples = 20

# 圆环 1
r1 = 1.0
theta1 = np.random.rand(n_samples) * 2 * np.pi
x1 = np.c_[r1 * np.cos(theta1), r1 * np.sin(theta1)]
y1 = np.ones(n_samples)   # label +1

# 圆环 2
r2 = 2.5
theta2 = np.random.rand(n_samples) * 2 * np.pi
x2 = np.c_[r2 * np.cos(theta2), r2 * np.sin(theta2)]
y2 = -np.ones(n_samples)  # label -1

# 合并
X_train = np.vstack([x1, x2])
y_train = np.hstack([y1, y2])

# -----------------------------
# 2. 定义核函数（用代码里的 RBF 核）
# -----------------------------
def rbf_kernel(x, z, gamma=1.0):
    diff = x - z
    return np.exp(-gamma * np.dot(diff, diff))

# -----------------------------
# 3. 训练 KernelSVM
# -----------------------------
svm = KernelSVM(kernel=rbf_kernel, C=1.0, epochs=10)  # 小样本，epoch 不用太大
svm.fit(X_train, y_train)

# -----------------------------
# 4. 预测新样本
# -----------------------------
X_test = np.array([
    [0.0, 0.0],   # 中心，应该属于 +1
    [3.0, 0.0],   # 外圈，应该属于 -1
    [1.5, 1.5]    # 中间，可能靠近边界
])

y_pred = svm.predict(X_test)
print("预测结果:", y_pred)

# -----------------------------
# 5. 可视化训练集和预测点
# -----------------------------
plt.scatter(x1[:,0], x1[:,1], color='red', label='+1')
plt.scatter(x2[:,0], x2[:,1], color='blue', label='-1')
plt.scatter(X_test[:,0], X_test[:,1], color='green', marker='x', s=100, label='预测点')

plt.legend()
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('KernelSVM 示例预测')
plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体
plt.rcParams['axes.unicode_minus'] = False    # 解决负号显示问题
plt.show()
